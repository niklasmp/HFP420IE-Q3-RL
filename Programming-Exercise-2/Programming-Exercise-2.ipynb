{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import operator\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0Z8CBEAuApp"
   },
   "source": [
    "<h1 style=\"text-align: center;\">Monte-Carlo Learning</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1628517296866,
     "user": {
      "displayName": "Maximilian Hölscher",
      "photoUrl": "",
      "userId": "02596010900742344706"
     },
     "user_tz": -120
    },
    "id": "zLjwus20VUfU"
   },
   "outputs": [],
   "source": [
    "def every_visit_monte_carlo(iterations, alpha, gamma, exercise_name, policy):\n",
    "    val_map = {}\n",
    "    for round in range(iterations):\n",
    "        episode = generate_full_episode(exercise_name, policy)\n",
    "        g = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            step = episode[t]\n",
    "            state = step[0]\n",
    "            reward = step[2]\n",
    "            g = gamma * g + reward\n",
    "            val_map[state] = val_map.get(state, 0) +  alpha * (g - val_map.get(state, 0))\n",
    "    return val_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-t6r3wA2d0n"
   },
   "source": [
    "<h1 style=\"text-align: center;\">Temporal-Difference Learning</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4NR1Y9I2_j3"
   },
   "outputs": [],
   "source": [
    "def temporal_difference_zero(iterations, initial_state_value, alpha, gamma, exercise_name, policy):\n",
    "    val_map = {}\n",
    "    for round in range(iterations):\n",
    "        episode = generate_full_episode(exercise_name, policy)\n",
    "        for t in range(0, len(episode)-1):\n",
    "            # current step (t)\n",
    "            step = episode[t]\n",
    "            state = step[0]\n",
    "            reward = step[2]\n",
    "            \n",
    "            # next step (t+1)\n",
    "            next_step = episode[t + 1]\n",
    "            next_state = next_step[0]\n",
    "            \n",
    "            # v(s_t)\n",
    "            state_value = val_map.get(state, initial_state_value)\n",
    "            \n",
    "            # v(s_t+1)\n",
    "            next_state_value = val_map.get(next_state, initial_state_value)\n",
    "            \n",
    "            # Incremental update of value function\n",
    "            val_map[state] = state_value + alpha * (reward + (gamma * next_state_value))\n",
    "        \n",
    "        val_map[next_state] = next_state_value\n",
    "    return val_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">TD(λ)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference_lambda(iterations, initial_state_value, alpha, gamma, exercise_name, policy, lambd=0.5):\n",
    "    val_map = {}\n",
    "    e_map = {}\n",
    "    for round in range(iterations):\n",
    "        episode = generate_full_episode(exercise_name, policy)\n",
    "        for t in range(0, len(episode)-1):\n",
    "            # current step (t)\n",
    "            step = episode[t]\n",
    "            state = step[0]\n",
    "            reward = step[2]\n",
    "            \n",
    "            # next step (t+1)\n",
    "            next_step = episode[t + 1]\n",
    "            next_state = next_step[0]\n",
    "            \n",
    "            if state not in val_map:\n",
    "                val_map[state] = initial_state_value\n",
    "            if state not in e_map:\n",
    "                e_map[state] = initial_state_value\n",
    "            delta = reward + gamma * val_map.get(next_state, initial_state_value) - val_map.get(state, initial_state_value)\n",
    "            e_map[state] = e_map.get(state, initial_state_value) + 1\n",
    "            for key in val_map:\n",
    "                e_map[state] = gamma * lambd * e_map.get(state, initial_state_value)\n",
    "                val_map[state] = val_map.get(state, initial_state_value) + alpha * delta * e_map.get(state, initial_state_value)                                    \n",
    "        val_map[next_state] = val_map.get(next_state, initial_state_value)\n",
    "    return val_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random policy\n",
    "def random_policy(env, state):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_episode(exercise, policy):\n",
    "    env = gym.make(exercise)\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    while True:\n",
    "        action = policy(env, state)\n",
    "        # perform the action\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode.append([state, action, reward])\n",
    "        state = new_state\n",
    "        if done:\n",
    "            # append termination state to episode\n",
    "            episode.append([state, -1, 0])\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">OpenAI</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "episodes = 1000\n",
    "alpha = .01\n",
    "gamma = .9\n",
    "initialization_value = 0\n",
    "lambd = .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">FrozenLake-v0</h2>\n",
    "\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "*Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.*\n",
    "\n",
    "*The surface is described using a grid like the following:*\n",
    "\n",
    "\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "Source: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "<img src=\"https://images.squarespace-cdn.com/content/v1/584219d403596e3099e0ee9b/1582183856858-QTMYV4HGGOA8BJPN83CA/frozen_lake.jpg?format=500w\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal FrozenLake policy\n",
    "\n",
    "NOOP = -1\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "action_script = {\n",
    "    0: DOWN,\n",
    "    1: RIGHT,\n",
    "    2: DOWN,\n",
    "    3: LEFT,\n",
    "    4: DOWN,\n",
    "    5: NOOP,\n",
    "    6: DOWN,\n",
    "    7: NOOP,\n",
    "    8: RIGHT,\n",
    "    9: DOWN,\n",
    "    10: DOWN,\n",
    "    11: NOOP,\n",
    "    12: NOOP,\n",
    "    13: RIGHT,\n",
    "    14: RIGHT,\n",
    "    15: NOOP\n",
    "}\n",
    "\n",
    "def optimal_policy(env, state):\n",
    "    return action_script[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7WNzA5xEycS"
   },
   "outputs": [],
   "source": [
    "exercise = 'FrozenLake-v0'\n",
    "\n",
    "# Monte Carlo\n",
    "val_map = every_visit_monte_carlo(episodes, alpha, gamma, exercise, random_policy)\n",
    "print('MC - Random policy')\n",
    "print(f'{np.array([val_map[key] for key in sorted(val_map.keys())]).reshape(4, 4)}\\n')\n",
    "\n",
    "val_map = every_visit_monte_carlo(episodes, alpha, gamma, exercise, optimal_policy)\n",
    "print('MC - Optimal policy')\n",
    "print(f'{np.array([val_map[key] for key in sorted(val_map.keys())]).reshape(4, 4)}\\n')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# TD(0)\n",
    "val_map = temporal_difference_zero(episodes, initialization_value, alpha, gamma, exercise, random_policy)\n",
    "print('TD(0) - Random policy')\n",
    "print(f'{np.array([val_map[key] for key in sorted(val_map.keys())]).reshape(4, 4)}\\n')\n",
    "\n",
    "val_map = temporal_difference_zero(episodes, initialization_value, alpha, gamma, exercise, optimal_policy)\n",
    "print('TD(0) - Optimal policy')\n",
    "print(f'{np.array([val_map[key] for key in sorted(val_map.keys())]).reshape(4, 4)}\\n')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# TD(λ)\n",
    "val_map = temporal_difference_lambda(episodes, initialization_value, alpha, gamma, exercise, random_policy, lambd)\n",
    "print('TD(λ) - Random policy')\n",
    "print(f'{np.array([val_map[key] for key in sorted(val_map.keys())]).reshape(4, 4)}\\n')\n",
    "\n",
    "val_map = temporal_difference_lambda(episodes, initialization_value, alpha, gamma, exercise, optimal_policy, lambd)\n",
    "print('TD(λ) - Optimal policy')\n",
    "print(f'{np.array([val_map[key] for key in sorted(val_map.keys())]).reshape(4, 4)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Taxi-v3</h1>\n",
    "\n",
    "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "Source: https://gym.openai.com/envs/Taxi-v3/\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Taxi v3 policy with random movement and targeted pick up and drop of \n",
    "\n",
    "# see https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py (l 134 ff.)\n",
    "x_factor = 100\n",
    "y_factor = 20\n",
    "\n",
    "# Calculates the positional part of the state\n",
    "def compute_positional_state_part(x, y):\n",
    "    return x * x_factor + y * y_factor\n",
    "\n",
    "## Position\n",
    "POSITION_R = compute_positional_state_part(0, 0)\n",
    "POSITION_G = compute_positional_state_part(0, 4)\n",
    "POSITION_Y = compute_positional_state_part(4, 0)\n",
    "POSITION_B = compute_positional_state_part(4, 3)\n",
    "\n",
    "## Passenger locations\n",
    "PASSENGER_AT_R = 0\n",
    "PASSENGER_AT_G = 1\n",
    "PASSENGER_AT_Y = 2\n",
    "PASSENGER_AT_B = 3\n",
    "PASSENGER_IN_TAXI = 4\n",
    "\n",
    "## Destinations\n",
    "DEST_R = 0\n",
    "DEST_G = 1\n",
    "DEST_Y = 2\n",
    "DEST_B = 3\n",
    "\n",
    "## Actions\n",
    "SOUTH = 0\n",
    "NORTH = 1\n",
    "EAST = 2\n",
    "WEST = 3\n",
    "PICK_UP = 4\n",
    "DROP_OFF = 5\n",
    "\n",
    "action_script = {\n",
    "    POSITION_R + PASSENGER_AT_R * 4 + DEST_R: PICK_UP,  # State: 0\n",
    "    POSITION_R + PASSENGER_AT_R * 4 + DEST_G: PICK_UP,  # State: 1\n",
    "    POSITION_R + PASSENGER_AT_R * 4 + DEST_Y: PICK_UP,  # State: 2\n",
    "    POSITION_R + PASSENGER_AT_R * 4 + DEST_B: PICK_UP,  # State: 3\n",
    "    POSITION_G + PASSENGER_AT_G * 4 + DEST_R: PICK_UP,  # State: 84\n",
    "    POSITION_G + PASSENGER_AT_G * 4 + DEST_G: PICK_UP,  # State: 85\n",
    "    POSITION_G + PASSENGER_AT_G * 4 + DEST_Y: PICK_UP,  # State: 86\n",
    "    POSITION_G + PASSENGER_AT_G * 4 + DEST_B: PICK_UP,  # State: 87\n",
    "    POSITION_Y + PASSENGER_AT_Y * 4 + DEST_R: PICK_UP,  # State: 408\n",
    "    POSITION_Y + PASSENGER_AT_Y * 4 + DEST_G: PICK_UP,  # State: 409\n",
    "    POSITION_Y + PASSENGER_AT_Y * 4 + DEST_Y: PICK_UP,  # State: 410\n",
    "    POSITION_Y + PASSENGER_AT_Y * 4 + DEST_B: PICK_UP,  # State: 411\n",
    "    POSITION_B + PASSENGER_AT_B * 4 + DEST_R: PICK_UP,  # State: 472\n",
    "    POSITION_B + PASSENGER_AT_B * 4 + DEST_G: PICK_UP,  # State: 473\n",
    "    POSITION_B + PASSENGER_AT_B * 4 + DEST_Y: PICK_UP,  # State: 474\n",
    "    POSITION_B + PASSENGER_AT_B * 4 + DEST_B: PICK_UP,  # State: 475\n",
    "    POSITION_R + PASSENGER_IN_TAXI * 4 + DEST_R: DROP_OFF,  # State: 16\n",
    "    POSITION_G + PASSENGER_IN_TAXI * 4 + DEST_G: DROP_OFF,  # State: 97\n",
    "    POSITION_Y + PASSENGER_IN_TAXI * 4 + DEST_Y: DROP_OFF,  # State: 418\n",
    "    POSITION_B + PASSENGER_IN_TAXI * 4 + DEST_B: DROP_OFF  # State: 479\n",
    "}\n",
    "\n",
    "\n",
    "def improved_policy(env, state):\n",
    "    return action_script.get(state, random.choice([SOUTH, NORTH, EAST, WEST]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executes the three policy evaluation algorithms for a random and a improved policy in the Taxi-v3 environment and stores the calculated state values. The calculation may took a while due to the the high number of episodes and steps per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise = 'Taxi-v3'\n",
    "\n",
    "# Monte Carlo\n",
    "val_map_mc_random = every_visit_monte_carlo(episodes, alpha, gamma, exercise, random_policy)\n",
    "val_map_mc_improved = every_visit_monte_carlo(episodes, alpha, gamma, exercise, improved_policy)\n",
    "\n",
    "# TD(0)\n",
    "val_map_td0_random = temporal_difference_zero(episodes, initialization_value, alpha, gamma, exercise, random_policy)\n",
    "val_map_td0_improved = temporal_difference_zero(episodes, initialization_value, alpha, gamma, exercise, improved_policy)\n",
    "\n",
    "# TD(λ)\n",
    "val_map_tdl_random = temporal_difference_lambda(episodes, initialization_value, alpha, gamma, exercise, random_policy, lambd)\n",
    "val_map_tdl_improved = temporal_difference_lambda(episodes, initialization_value, alpha, gamma, exercise, improved_policy, lambd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\">Comparison of value-functions ... </h3>\n",
    "\n",
    "The following methods can be used to print certain state values of the Taxi-v3 environment in both policies with all evaluation algorithms. Requires that the above code has been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(exercise)\n",
    "\n",
    "def render(state):\n",
    "    env.env.s = state\n",
    "    env.render()\n",
    "    \n",
    "def print_state_values(state):\n",
    "    print(f'State #{state}')\n",
    "    render(state)\n",
    "    print(f'Policy   | Algorithm | Value-Function')\n",
    "    print(f'---------+-----------+---------------')\n",
    "    print(f'Random   | MC        | {val_map_mc_random.get(state, 0)}')\n",
    "    print(f'Random   | TD(0)     | {val_map_td0_random.get(state, 0)}')\n",
    "    print(f'Random   | TD(λ)     | {val_map_tdl_random.get(state, 0)}')\n",
    "    print(f'---------+-----------+---------------')\n",
    "    print(f'Improved | MC        | {val_map_mc_improved.get(state, 0)}')\n",
    "    print(f'Improved | TD(0)     | {val_map_td0_improved.get(state, 0)}')\n",
    "    print(f'Improved | TD(λ)     | {val_map_tdl_improved.get(state, 0)}')\n",
    "    print(f'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\">... for all states</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, env.nS):\n",
    "    print_state_values(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\">... for a specific state</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 97\n",
    "print_state_values(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\">... for the best states</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best states\n",
    "\n",
    "print('- Random policy (best) -\\n')\n",
    "mc_random_best = max(val_map_mc_random.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'MC   : #{mc_random_best}')\n",
    "print(f'value: {val_map_mc_random[mc_random_best]}')\n",
    "render(mc_random_best)\n",
    "\n",
    "td0_random_best = max(val_map_td0_random.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'TD(0): #{td0_random_best}')\n",
    "print(f'value: {val_map_td0_random[td0_random_best]}')\n",
    "render(td0_random_best)\n",
    "\n",
    "tdl_random_best = max(val_map_tdl_random.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'TD(λ): #{tdl_random_best}')\n",
    "print(f'value: {val_map_tdl_random[tdl_random_best]}')\n",
    "render(tdl_random_best)\n",
    "\n",
    "\n",
    "print('- Improved random policy (best) -\\n')\n",
    "mc_improved_best = max(val_map_mc_improved.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'MC   : #{mc_improved_best}')\n",
    "print(f'value: {val_map_mc_improved[mc_improved_best]}')\n",
    "render(mc_improved_best)\n",
    "\n",
    "td0_improved_best = max(val_map_td0_improved.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'TD(0): #{td0_improved_best}')\n",
    "print(f'value: {val_map_td0_improved[td0_improved_best]}')\n",
    "render(td0_improved_best)\n",
    "\n",
    "tdl_improved_best = max(val_map_tdl_improved.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'TD(λ): #{tdl_improved_best}')\n",
    "print(f'value: {val_map_tdl_improved[tdl_improved_best]}')\n",
    "render(tdl_improved_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\">... for the worst states</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worst states\n",
    "\n",
    "print('- Random policy (worst) -\\n')\n",
    "mc_random_worst = min(val_map_mc_random.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'MC   : #{mc_random_worst}')\n",
    "print(f'value: {val_map_mc_random[mc_random_worst]}')\n",
    "render(mc_random_worst)\n",
    "\n",
    "td0_random_worst = min(val_map_td0_random.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'TD(0): #{td0_random_worst}')\n",
    "print(f'value: {val_map_td0_random[td0_random_worst]}')\n",
    "render(td0_random_worst)\n",
    "\n",
    "tdl_random_worst = min(val_map_tdl_random.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'TD(λ): #{tdl_random_worst}')\n",
    "print(f'value: {val_map_tdl_random[tdl_random_worst]}')\n",
    "render(tdl_random_worst)\n",
    "\n",
    "\n",
    "print('- Improved random policy (worst) -\\n')\n",
    "mc_improved_worst = min(val_map_mc_improved.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'MC   : #{mc_improved_worst}')\n",
    "print(f'value: {val_map_mc_improved[mc_improved_worst]}')\n",
    "render(mc_improved_worst)\n",
    "\n",
    "td0_improved_worst = min(val_map_td0_improved.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'TD(0): #{td0_improved_worst}')\n",
    "print(f'value: {val_map_td0_improved[td0_improved_worst]}')\n",
    "render(td0_improved_worst)\n",
    "\n",
    "tdl_improved_worst = min(val_map_tdl_improved.items(), key=operator.itemgetter(1))[0]\n",
    "print(f'TD(λ): #{tdl_improved_worst}')\n",
    "print(f'value: {val_map_tdl_improved[tdl_improved_worst]}')\n",
    "render(tdl_improved_worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOa8xzkEI/2Y9Ip8Uez16iN",
   "collapsed_sections": [],
   "name": "RL - Model free learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
