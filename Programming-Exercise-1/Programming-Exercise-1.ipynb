{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc90ffc6",
   "metadata": {},
   "source": [
    "# Action Selection Strategies Comparison\n",
    "\n",
    "We will compare the following action selection strategies\n",
    "1. ε-greedy\n",
    "1. Optimistic Initial Estimates\n",
    "1. Softmax\n",
    "1. Upper-Confidence-Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81005e",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6604185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7366f7",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e985b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Parameters\n",
    "initial_winning_probs = [0.1, 0.3, 0.6, 0.4, 0.1, 0.4, 0.69, 0.71, 0.5, 0.2]\n",
    "k = np.size(initial_winning_probs)\n",
    "\n",
    "# Comparison Parameters\n",
    "iterations_for_average = 100\n",
    "number_of_plays = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_bandid(action: int):\n",
    "    times_won = 0\n",
    "    for _ in range(10):\n",
    "        if random.random() < initial_winning_probs[action]:\n",
    "            times_won += 1\n",
    "    return times_won"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde4d64",
   "metadata": {},
   "source": [
    "# ε-greedy\n",
    "### Idea\n",
    "The *ε-greedy* strategy combines exploration and exploitation. The agent maintains a list with the average reward of each action.\n",
    "Usually the agent will choose the action with the highest estimated value (exploitation). \n",
    "In some cases the agent chooses a random action to explore the environment (exploration).  \n",
    "\n",
    "\n",
    "### Parameter\n",
    "- $ε$ is the probability of choosing a random action instead of the action with the highest estimated value.\n",
    "\n",
    "\n",
    "### Formula\n",
    "\n",
    "\\begin{equation}\n",
    "A_t = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        argmax_a\\big[Q_t(a)\\big] & with\\:probability\\:1-ε \\\\\n",
    "        random\\:action & with\\:probability\\:ε\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "- $Q_t(a)$ is the estimated value of action $a$ at time step $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_epsilon_greedy(epsilon, number_of_plays=number_of_plays):\n",
    "    mean_rewards = [0]\n",
    "    q = np.zeros(k)\n",
    "    count = np.zeros(k)\n",
    "    for t in range(1, number_of_plays):\n",
    "        a = np.argmax(q)\n",
    "        if random.random() < epsilon:\n",
    "            a = np.random.randint(k)\n",
    "        reward = play_bandid(a)\n",
    "        count[a] += 1\n",
    "        mean_reward = mean_rewards[-1] + ((reward - mean_rewards[-1]) / t)\n",
    "        mean_rewards.append(mean_reward)\n",
    "        q[a] = q[a] + (reward - q[a]) / count[a]\n",
    "    return mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654511ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_plot():\n",
    "    epsilon_averages_1_128 = []\n",
    "    epsilon_averages_1_64 = []\n",
    "    epsilon_averages_1_32 = []\n",
    "    epsilon_averages_1_16 = []\n",
    "    epsilon_averages_1_8 = []\n",
    "    epsilon_averages_1_4 = []\n",
    "    epsilon_averages_1_2 = []\n",
    "    epsilon_averages_1 = []\n",
    "    [ epsilon_averages_1_128.append(np.average(learn_epsilon_greedy(1/128))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_averages_1_64.append(np.average(learn_epsilon_greedy(1/64))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_averages_1_32.append(np.average(learn_epsilon_greedy(1/32))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_averages_1_16.append(np.average(learn_epsilon_greedy(1/16))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_averages_1_8.append(np.average(learn_epsilon_greedy(1/8))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_averages_1_4.append(np.average(learn_epsilon_greedy(1/4))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_averages_1_2.append(np.average(learn_epsilon_greedy(1/2))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_averages_1.append(np.average(learn_epsilon_greedy(1))) for x in range(iterations_for_average) ]\n",
    "    epsilon_greedy_epsilons = [1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1]\n",
    "    epsilon_greedy_averages = [\n",
    "        np.average(epsilon_averages_1_128),\n",
    "        np.average(epsilon_averages_1_64),\n",
    "        np.average(epsilon_averages_1_32),\n",
    "        np.average(epsilon_averages_1_16),\n",
    "        np.average(epsilon_averages_1_8),\n",
    "        np.average(epsilon_averages_1_4),\n",
    "        np.average(epsilon_averages_1_2),\n",
    "        np.average(epsilon_averages_1),\n",
    "    ]\n",
    "    return epsilon_greedy_epsilons, epsilon_greedy_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "line1, = plt.plot(learn_epsilon_greedy(0.05), label=\"ε=0.05\")\n",
    "line2, = plt.plot(learn_epsilon_greedy(0.1), label=\"ε=0.1\")\n",
    "line3, = plt.plot(learn_epsilon_greedy(0.2), label=\"ε=0.2\")\n",
    "plt.legend()\n",
    "plt.title('Different Exploration Rates (ε)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "epsilon_greedy_epsilons, epsilon_greedy_averages = epsilon_greedy_plot()\n",
    "plt.plot(epsilon_greedy_epsilons, epsilon_greedy_averages, \"r-\", label=\"ε-greedy (ε)\")\n",
    "plt.legend()\n",
    "plt.semilogx(base=2)\n",
    "plt.xticks(\n",
    "    [1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1], \n",
    "    ['1/128', '1/64', '1/32', '1/16', '1/8', '1/4', '1/2', '1'])\n",
    "plt.ylabel(f'Average reward over first {number_of_plays} steps')\n",
    "plt.title(f'Comparison ε values \\n(averaging over {iterations_for_average} executions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c9887",
   "metadata": {},
   "source": [
    "# Optimistic Initial Estimates\n",
    "### Idea\n",
    "With the *Optimistic Initial Estimates* we try to encourage exploring actions. This works even in combination with a plain greedy approach. The idea behind the Optimistic Initial Estimates is that we initialize all the $Q(a)$ with values that are in the range or even a little bit higher than the mean rewards. \n",
    "\n",
    "This leads to the behaviour that the agent picks an action and then probably gets disappointed by the reward because it's very likely that the initial value for this action has been higher. In the next iteration the agent is likely to pick a different action. The result is an agent that is encouraged to try different actions in the beginning before the value estimates converge. \n",
    "\n",
    "### Parameter\n",
    "- $initial\\:value$ is used for initializing the estimates $Q_0(a)$. This parameter should usually stay in the range of the mean rewards or even a little bit higher.\n",
    "\n",
    "### Formula\n",
    "$$\\forall a:A: Q_0(a) = initial\\:value$$\n",
    "- $A$ is the set of all actions the agent can pick.\n",
    "- $Q_0(a)$ is the estimated value of action $a$ at time step $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_weighted_average_with_initial_values(initial_q_values, number_of_plays = number_of_plays):\n",
    "    mean_rewards = [0]\n",
    "    alpha = 0.1\n",
    "    q = np.full(k, initial_q_values, dtype=float)\n",
    "    count = np.zeros(k)\n",
    "    for t in range(1, number_of_plays):\n",
    "        a = np.argmax(q)\n",
    "        reward = play_bandid(a)\n",
    "        count[a] += 1\n",
    "        mean_reward = mean_rewards[-1] + 1/100 * (reward - mean_rewards[-1])\n",
    "        mean_rewards.append(mean_reward)\n",
    "        q[a] = q[a] + alpha * (reward - q[a])\n",
    "    return mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8196658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_values_plot():\n",
    "    epsilon_optimistic_averages_1_2 = []\n",
    "    epsilon_optimistic_averages_1 = []\n",
    "    epsilon_optimistic_averages_2 = []\n",
    "    epsilon_optimistic_averages_4 = []\n",
    "    epsilon_optimistic_averages_8 = []\n",
    "    epsilon_optimistic_averages_10 = []\n",
    "    [ epsilon_optimistic_averages_1_2.append(np.average(learn_weighted_average_with_initial_values(1/2))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_optimistic_averages_1.append(np.average(learn_weighted_average_with_initial_values(1))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_optimistic_averages_2.append(np.average(learn_weighted_average_with_initial_values(2))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_optimistic_averages_4.append(np.average(learn_weighted_average_with_initial_values(4))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_optimistic_averages_8.append(np.average(learn_weighted_average_with_initial_values(8))) for x in range(iterations_for_average) ]\n",
    "    [ epsilon_optimistic_averages_10.append(np.average(learn_weighted_average_with_initial_values(10))) for x in range(iterations_for_average) ]\n",
    "    epsilon_optimistic_initializations = [1/2, 1, 2, 4, 8, 10]\n",
    "    epsilon_optimistic_averages = [\n",
    "        np.average(epsilon_optimistic_averages_1_2),\n",
    "        np.average(epsilon_optimistic_averages_1),\n",
    "        np.average(epsilon_optimistic_averages_2),\n",
    "        np.average(epsilon_optimistic_averages_4),\n",
    "        np.average(epsilon_optimistic_averages_8),\n",
    "        np.average(epsilon_optimistic_averages_10)]\n",
    "    return epsilon_optimistic_initializations, epsilon_optimistic_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21359990",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(learn_weighted_average_with_initial_values(10), label=\"initial_values=10, α=0.1\")\n",
    "plt.plot(learn_weighted_average_with_initial_values(8), label=\"initial_values=8, α=0.1\")\n",
    "plt.plot(learn_weighted_average_with_initial_values(4), label=\"initial_values=4, α=0.1\")\n",
    "plt.plot(learn_weighted_average_with_initial_values(1), label=\"initial_values=1, α=0.1\")\n",
    "plt.legend()\n",
    "plt.title('Different initial values vs. using ε-Greedy-Approach')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbf506",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "epsilon_optimistic_initializations, epsilon_optimistic_averages = optimistic_values_plot()\n",
    "\n",
    "plt.plot(epsilon_optimistic_initializations, epsilon_optimistic_averages, \"k-\", label=\"optimistic initialization (initial value, α=0.1)\")\n",
    "plt.legend()\n",
    "plt.semilogx(base=2)\n",
    "plt.xticks(\n",
    "    [1/2, 1, 2, 4, 8, 10], \n",
    "    ['1/2', '1', '2', '4', '8', '10'])\n",
    "plt.ylabel(f'Average reward over first {number_of_plays} steps')\n",
    "plt.title(f'Comparison initial action values \\n(averaging over {iterations_for_average} executions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b757fd20",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "### Idea\n",
    "The idea behind *softmax* is that we rate actions with a good performance higher and discriminate actions that performs poorly. *Softmax* create a probability distribution where actions with a high estimated value have a high probability.\n",
    "\n",
    "### Parameter\n",
    "- $τ$ is called the temperature. A higher temperature leads to more similar probabilities for actions.\n",
    "\n",
    "### Formula\n",
    "$$\\pi_t(a) = Pr\\{A_t = a\\} = \\frac{e^{Q_t(a)/τ}}{\\sum\\limits_{i=1}^{n} e^{Q_t(i)/τ}}$$\n",
    "- $n$ is the number of actions\n",
    "- $Q_t(a)$ is the estimated value of action $a$ at time step $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0cfe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(q, tau):\n",
    "    return (np.exp(q / tau) / np.sum(np.exp(q / tau)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_softmax(tau, number_of_plays = number_of_plays):\n",
    "    mean_rewards = [0]\n",
    "    q = np.zeros(k)\n",
    "    count = np.zeros(k)\n",
    "    for t in range(1, number_of_plays):\n",
    "        pi_t = softmax(q, tau)\n",
    "        a = np.random.choice(np.arange(k), p=pi_t)\n",
    "        reward = play_bandid(a)\n",
    "        count[a] += 1\n",
    "        mean_reward = mean_rewards[-1] + ((reward - mean_rewards[-1]) / t)\n",
    "        mean_rewards.append(mean_reward)\n",
    "        q[a] = q[a] + (reward - q[a]) / count[a]\n",
    "    return mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "line1, = plt.plot(learn_softmax(0.8, 2000), label=\"τ=0.8\")\n",
    "line2, = plt.plot(learn_softmax(1.0, 2000), label=\"τ=1.0\")\n",
    "line3, = plt.plot(learn_softmax(1.2, 2000), label=\"τ=1.2\")\n",
    "plt.legend()\n",
    "plt.title('Softmax Action Selection with different Temperatures (τ)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_plot():\n",
    "    softmax_averages_1_16 = []\n",
    "    softmax_averages_1_8 = []\n",
    "    softmax_averages_1_4 = []\n",
    "    softmax_averages_1_2 = []\n",
    "    softmax_averages_3_4 = []\n",
    "    softmax_averages_1 = []\n",
    "    softmax_averages_5_4 = []\n",
    "    softmax_averages_3_2 = []\n",
    "    softmax_averages_2 = []\n",
    "    softmax_averages_4 = []\n",
    "    [ softmax_averages_1_16.append(np.average(learn_softmax(1/16))) for x in range(iterations_for_average) ]\n",
    "    [ softmax_averages_1_8.append(np.average(learn_softmax(1/8))) for x in range(iterations_for_average) ]\n",
    "    [ softmax_averages_1_4.append(np.average(learn_softmax(1/4))) for x in range(iterations_for_average) ]\n",
    "    [ softmax_averages_1_2.append(np.average(learn_softmax(1/2))) for x in range(iterations_for_average) ]\n",
    "    [ softmax_averages_3_4.append(np.average(learn_softmax(3/4))) for x in range(iterations_for_average) ]\n",
    "    [ softmax_averages_1.append(np.average(learn_softmax(1))) for x in range(iterations_for_average) ]\n",
    "    [ softmax_averages_5_4.append(np.average(learn_softmax(5/4))) for x in range(iterations_for_average) ]\n",
    "    [ softmax_averages_3_2.append(np.average(learn_softmax(3/2))) for x in range(iterations_for_average) ]\n",
    "    [ softmax_averages_2.append(np.average(learn_softmax(2))) for x in range(iterations_for_average) ]\n",
    "    [ softmax_averages_4.append(np.average(learn_softmax(4))) for x in range(iterations_for_average) ]\n",
    "    softmax_taus = [1/16, 1/8, 1/4, 1/2, 3/4, 1, 5/4, 3/2, 2, 4]\n",
    "    softmax_averages = [\n",
    "        np.average(softmax_averages_1_16),\n",
    "        np.average(softmax_averages_1_8),\n",
    "        np.average(softmax_averages_1_4),\n",
    "        np.average(softmax_averages_1_2),\n",
    "        np.average(softmax_averages_3_4),\n",
    "        np.average(softmax_averages_1),\n",
    "        np.average(softmax_averages_5_4),\n",
    "        np.average(softmax_averages_3_2),\n",
    "        np.average(softmax_averages_2),\n",
    "        np.average(softmax_averages_4)]\n",
    "    return softmax_taus, softmax_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "softmax_taus, softmax_averages = softmax_plot()\n",
    "plt.plot(softmax_taus, softmax_averages, \"c-\", label=\"Softmax (τ)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.semilogx(base=2)\n",
    "plt.xticks(\n",
    "    softmax_taus, \n",
    "    ['1/16', '1/8', '1/4', '1/2', '0.75', '1', '1.25', '1.5', '2', '4'])\n",
    "plt.ylabel(f'Average reward over first {number_of_plays} steps')\n",
    "plt.title(f'Comparison τ values \\n(averaging over {iterations_for_average} executions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819bc53",
   "metadata": {},
   "source": [
    "# Upper-Confidence-Bound\n",
    "### Idea\n",
    "With the *Upper-Confidence-Bound* we try to encourage exploring actions that were not explored very often. Similar to the other strategies we also have a portion for exploration and exploitation that can be controlled by a hyperparameter.\n",
    "\n",
    "### Parameter\n",
    "- $c$ is a confidence value that controls the level of exploration. This parameter should be in the same range as the mean reward $Q_t(a)$ because otherwise its impact would be too small and the likelihood of exploration would decrease.\n",
    "\n",
    "### Formula\n",
    "$$A_t = argmax_a\\Bigg[ Q_t(a) + c\\sqrt\\frac{\\ln t}{N_t(a)}\\Bigg]$$\n",
    "- $Q_t(a)$ is the estimated value of action $a$ at time step $t$.\n",
    "- $N_t(a)$ is the number of times that action $a$ has been selected, prior to time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_confidence_bound(q, c, t, count):\n",
    "    return np.argmax(q + c * np.sqrt(np.log(t) / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_uppper_confidence_bound(c, number_of_plays = number_of_plays):\n",
    "    mean_rewards = [0]\n",
    "    q = np.zeros(k)\n",
    "    count = np.full(k, 1)\n",
    "    for t in range(1, number_of_plays):\n",
    "        a = upper_confidence_bound(q, c, t, count)\n",
    "        reward = play_bandid(a)\n",
    "        count[a] += 1\n",
    "        mean_reward = mean_rewards[-1] + ((reward - mean_rewards[-1]) / t)\n",
    "        mean_rewards.append(mean_reward)\n",
    "        q[a] = q[a] + ((reward - q[a]) / count[a])\n",
    "    return mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(learn_uppper_confidence_bound(4.0), label=\"c=4.0\")\n",
    "plt.plot(learn_uppper_confidence_bound(3.0), label=\"c=3.0\")\n",
    "plt.plot(learn_uppper_confidence_bound(2.0), label=\"c=2.0\")\n",
    "plt.plot(learn_uppper_confidence_bound(1.0), label=\"c=1.0\")\n",
    "plt.legend()\n",
    "plt.title('Different c values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ac297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb_plot():\n",
    "    ucb_averages_1_2 = []\n",
    "    ucb_averages_1 = []\n",
    "    ucb_averages_2 = []\n",
    "    ucb_averages_4 = []\n",
    "    ucb_averages_8 = []\n",
    "    ucb_averages_10 = []\n",
    "    [ ucb_averages_1_2.append(np.average(learn_uppper_confidence_bound(1/2))) for x in range(iterations_for_average) ]\n",
    "    [ ucb_averages_1.append(np.average(learn_uppper_confidence_bound(1))) for x in range(iterations_for_average) ]\n",
    "    [ ucb_averages_2.append(np.average(learn_uppper_confidence_bound(2))) for x in range(iterations_for_average) ]\n",
    "    [ ucb_averages_4.append(np.average(learn_uppper_confidence_bound(4))) for x in range(iterations_for_average) ]\n",
    "    [ ucb_averages_8.append(np.average(learn_uppper_confidence_bound(8))) for x in range(iterations_for_average) ]\n",
    "    [ ucb_averages_10.append(np.average(learn_uppper_confidence_bound(10))) for x in range(iterations_for_average) ]\n",
    "    ucb_cs = [1/2, 1, 2, 4, 8, 10]\n",
    "    ucb_averages = [\n",
    "        np.average(ucb_averages_1_2),\n",
    "        np.average(ucb_averages_1),\n",
    "        np.average(ucb_averages_2),\n",
    "        np.average(ucb_averages_4),\n",
    "        np.average(ucb_averages_8),\n",
    "        np.average(ucb_averages_10)]\n",
    "    return ucb_cs, ucb_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "ucb_cs, ucb_averages = ucb_plot()\n",
    "\n",
    "plt.plot(ucb_cs, ucb_averages, \"k-\", label=\"optimistic initialization (initial value, α=0.1)\")\n",
    "plt.legend()\n",
    "plt.semilogx(base=2)\n",
    "plt.xticks(\n",
    "    [1/2, 1, 2, 4, 8, 10], \n",
    "    ['1/2', '1', '2', '4', '8', '10'])\n",
    "plt.ylabel(f'Average reward over first {number_of_plays} steps')\n",
    "plt.title(f'Comparison initial action values \\n(averaging over {iterations_for_average} executions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5ce1e",
   "metadata": {},
   "source": [
    "# Comparison Action Selection Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd06bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Epsilon-greedy\n",
    "epsilon_greedy_epsilons, epsilon_greedy_averages = epsilon_greedy_plot()\n",
    "plt.plot(epsilon_greedy_epsilons, epsilon_greedy_averages, \"r-\", label=\"ε-greedy (ε)\")\n",
    "\n",
    "# Weighted Average with Optimistic Initialization\n",
    "epsilon_optimistic_initializations, epsilon_optimistic_averages = optimistic_values_plot()\n",
    "plt.plot(epsilon_optimistic_initializations, epsilon_optimistic_averages, \"k-\", label=\"optimistic initialization (initial value, α=0.1, ε=0)\")\n",
    "\n",
    "# UCB\n",
    "ucb_cs, ucb_averages = ucb_plot()\n",
    "plt.plot(ucb_cs, ucb_averages, \"b-\", label=\"UCB (c)\")\n",
    "\n",
    "# Softmax\n",
    "softmax_taus, softmax_averages = softmax_plot()\n",
    "plt.plot(softmax_taus, softmax_averages, \"g-\", label=\"Softmax (τ)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.semilogx(base=2)\n",
    "plt.xticks(\n",
    "    [1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 10], \n",
    "    ['1/128', '1/64', '1/32', '1/16', '1/8', '1/4', '1/2', '1', '2', '4', '8', '10'])\n",
    "plt.ylabel(f'Average reward over first {number_of_plays} steps')\n",
    "plt.title(f'Comparison Action Selection Strategies \\n(averaging over {iterations_for_average} executions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f82a8",
   "metadata": {},
   "source": [
    "# Performance / Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78570c86",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">ε-greedy</span>\n",
    "\n",
    "The *ε-greedy* performs acceptable in the range between $ε=\\frac{1}{16}$ and $ε=\\frac{1}{4}$. With lower ε values the share of explorations is to low therefore the valuable actions are found only later. With a higher ε value the agent learns the *real* action values more quickly. However the agent will still choose random actions during the execution. This leeds to a lower average reward.\n",
    "\n",
    "The best performance can be achieved with $ε=\\frac{1}{8}$. With that configuration the agent has a good relation of exploitation and exploration. \n",
    "\n",
    "The agent reaches an average reward of $4$ with $ε=1$. In this case the agent will choose a random action a each step. $4$ is the average of all winning probs (0,4) multiplied by k (10) - the number of play per step.\n",
    "\n",
    "**Domain of definition** In this environment the $ε$ values should be in the range between $\\frac{1}{16}$ and $\\frac{1}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fad4e6",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-weight:bold\">Optimistic Initial Estimates</span>\n",
    "\n",
    "The *Optimistic Initial Estimates* starts to perform reasonable with $initial\\:value=4$. This is because the exploration portion increases with $initial\\:value$ which leads to the agent is more likely to pick an action that has not been picked often yet.\n",
    "\n",
    "The highest average reward can be achieved with $initial\\:value=8$. One possible reason for performing best at this value might be that the value is close to the best reward of all bandits. Then its quite likely that the agent has picked every action at least once and has an estimate for each action.\n",
    "\n",
    "As $initial\\:value$ converges to inifinity the expected average reward is 4 (the average reward of all bandits) because the agent will pick each action equally likely.\n",
    "\n",
    "**Domain of definition** The $initial\\:value$ should be in the range of the expected best reward of the bandits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458f9e5",
   "metadata": {},
   "source": [
    "<span style=\"color:green; font-weight:bold\">Softmax</span>\n",
    "\n",
    "The *Softmax* performs reasonable with $τ$ between $0.75$ and $1.5$. Lower $τ$ values reduce the exploitation therefore more steps are needed to find the best actions. With higher $τ$ values the probability distribution is too similar and actions that performs poorly are not discriminated enough. The best performance is reached with $τ=1$.\n",
    "\n",
    "**Domain of definition** In this environment the temperature $τ$ should be between $0.75$ and $1.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0e576",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-weight:bold\">Upper-Confidence-Bound</span>\n",
    "\n",
    "The *Upper-Confidence-Bound* starts to perform reasonable with $c=2$. This is because the exploration portion increases with $c$ which leads to the agent is more likely to pick an action that has not been picked often yet. \n",
    "\n",
    "The highest average reward can be achieved with $c=4$. One possible reason for performing best at this value is that the value might be equal to the average reward of all bandits.\n",
    "\n",
    "As $c$ increases from that point on and converges to the best case reward of 10 the exploration portion gets to high which leads to almost randomly picking actions. As $c$ converges to inifinity the expected average reward is 4 (the average reward of all bandits) because the agent will pick each action equally likely.\n",
    "\n",
    "**Domain of definition** The $c$ parameter should be in the range of the expected average reward of the bandits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
