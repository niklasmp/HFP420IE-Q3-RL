{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c697c57",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Taxi-v3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c93ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59314e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "exercise = 'Taxi-v3'\n",
    "\n",
    "# Hyperparameter\n",
    "alpha = .1\n",
    "epsilon = .1\n",
    "gamma = .95\n",
    "episodes = 10000\n",
    "\n",
    "# Visualization\n",
    "rolling_mean_size = 1\n",
    "marker = ','\n",
    "line = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d333a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_epsilon_greedy(state, epsilon, q_map, actions):\n",
    "    if random.uniform(0, 1) <= epsilon:\n",
    "        return random.randrange(0, actions)\n",
    "    else:\n",
    "        return np.argmax(q_map[state])\n",
    "\n",
    "\n",
    "def choose_action_greedy(state, q_map, actions):\n",
    "    return choose_action_epsilon_greedy(state, 0, q_map, actions)\n",
    "\n",
    "\n",
    "def plot_episode_statistics(data, episodes, label):\n",
    "    df = pd.DataFrame({'x': np.arange(1, episodes + 1), 'y': data})\n",
    "    rolling_means = df.rolling(rolling_mean_size).mean()\n",
    "    plt.plot('x', 'y', f'{marker}{line}', data=rolling_means, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58eddd2",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Dyna-Q</h2>\n",
    "The Dyna-Q algorithm is an extension of the Q-Learnings algorithm with a planning phase. \n",
    "During the execution of the episodes we create an internal MDP from the observed behaviour.\n",
    "That MDP has two functions $ \\hat{P}_{s, s'}^a $ and $ \\hat{R}_s^a $.\n",
    "$ \\hat{P}_{s, s'}^a $ is the probability, that action $ a $ choosen in state $ s $ leeds to state $ s' $.\n",
    "$ \\hat{R}_s^a $ represents the expected reward if actionn $ a $ is choosen in step $ s $.\n",
    "\n",
    "These two functions are maintained after each step in the environment.\n",
    "After each step we execute $n $ planning steps where we choose a random state action pair that we observed earlier. \n",
    "With our created MDP we simulate a transition from $ s $ with action $ a $ and update the q-values with the gained reward.\n",
    "\n",
    "That planning phase has the advantage that less episodes are needed to find the <i>real</i> Q-values.\n",
    "\n",
    "<h2 style=\"text-align: center;\">Dyna-Q+</h2>\n",
    "Dyna-Q+ is an extension of the Dyna-Q algotithm to enable the agent to learn changes in a non-stationary environment.\n",
    "The difference is that we add an exploration bonus to the received reward if the transition wasn't executed for a long time.\n",
    "The algorithms stores the information when a transition was executed the last time. \n",
    "While updating the q-values we clculate the following values and add it to the received reward:\n",
    "$$ \\kappa*\\sqrt{\\tau} $$.\n",
    "$ \\kappa $ is a small constant factor. \n",
    "$ \\tau $ is the number of steps that have been executed since the transitions was executed the last time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc49355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dyna-Q(+)\n",
    "n = 5 # planning steps\n",
    "kappa = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1cf330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyna_q_plus(exercise, number_of_episodes, epsilon, gamma, alpha, planning_steps, kappa):\n",
    "    env = gym.make(exercise)\n",
    "    steps_per_episode = []\n",
    "\n",
    "    # Initialization\n",
    "    q_map = np.zeros((env.nS, env.nA))\n",
    "\n",
    "    transition_probabilities = np.zeros((env.nS, env.nS, env.nA))\n",
    "    transition_rewards = np.zeros((env.nS, env.nA))\n",
    "    state_action_state_transitions = np.zeros((env.nS, env.nS, env.nA))\n",
    "    state_action_transitions = np.zeros((env.nS, env.nA))\n",
    "    state_action_rewards = np.zeros((env.nS, env.nA))\n",
    "\n",
    "    # Dyna-Q+ Extension\n",
    "    last_visit = np.zeros((env.nS, env.nA))\n",
    "    visited_states = set()\n",
    "\n",
    "    global_step_counter = 0\n",
    "    for episode in range(1, number_of_episodes + 1):\n",
    "        episode_finished = False\n",
    "        s = env.reset()\n",
    "        step_counter = 0\n",
    "\n",
    "        while not episode_finished:\n",
    "            step_counter += 1\n",
    "            global_step_counter += 1\n",
    "            \n",
    "            a = choose_action_epsilon_greedy(s, epsilon, q_map, env.nA)\n",
    "\n",
    "            s_next, reward, done, _ = env.step(a)\n",
    "            episode_finished = done\n",
    "\n",
    "            a_next = choose_action_greedy(s_next, q_map, env.nA)\n",
    "\n",
    "            # Dyna-Q+ Extension\n",
    "            if last_visit[s, a] != 0:\n",
    "                reward = reward + kappa * np.sqrt(global_step_counter - last_visit[s, a])\n",
    "\n",
    "            q_map[s, a] = q_map[s, a] + alpha * (reward + gamma * q_map[s_next, a_next] - q_map[s, a])\n",
    "\n",
    "            last_visit[s, a] = global_step_counter\n",
    "\n",
    "            # Update model\n",
    "            state_action_transitions[s, a] = state_action_transitions[s, a] + 1\n",
    "            state_action_state_transitions[s, s_next, a] = state_action_state_transitions[s, s_next, a] + 1\n",
    "            state_action_rewards[s, a] = state_action_rewards[s, a] + reward\n",
    "            \n",
    "            list_of_next_states = state_action_state_transitions[s, :, a]\n",
    "            for element in range(len(list_of_next_states)):\n",
    "                transition_probabilities[s, element, a] = state_action_state_transitions[s, element, a] / state_action_transitions[s, a]\n",
    "            \n",
    "            transition_rewards[s, a] = state_action_rewards[s, a] / state_action_transitions[s, a]\n",
    "            visited_states.add((s, a))\n",
    "\n",
    "            # Planning\n",
    "            for n in range(planning_steps):\n",
    "                (s_learn, a_learn) = random.choice(list(visited_states))\n",
    "\n",
    "                target_states = transition_probabilities[s_learn, :, a_learn]\n",
    "\n",
    "                s_learn_next = np.random.choice(np.arange(target_states.size), p=target_states)\n",
    "                r_learn = transition_rewards[s_learn, a_learn]\n",
    "\n",
    "                # Dyna-Q+ Extension\n",
    "                r_learn = r_learn + kappa * np.sqrt(global_step_counter - last_visit[s_learn, a_learn])\n",
    "\n",
    "                a_learn_next = choose_action_greedy(s_learn_next, q_map, env.nA)\n",
    "                q_map[s_learn, a_learn] = q_map[s_learn, a_learn] + alpha * (\n",
    "                            r_learn + gamma * q_map[s_learn_next, a_learn_next] - q_map[s_learn, a_learn])\n",
    "            s = s_next\n",
    "        steps_per_episode.append(step_counter)\n",
    "    return steps_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815176ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plot_episode_statistics(dyna_q_plus(exercise, episodes, epsilon, gamma, alpha, 0, 0), episodes, f'Q-Learning')\n",
    "plot_episode_statistics(dyna_q_plus(exercise, episodes, epsilon, gamma, alpha, n, 0), episodes, f'Dyna-Q: {n} planning steps')\n",
    "plot_episode_statistics(dyna_q_plus(exercise, episodes, epsilon, gamma, alpha, n, kappa), episodes, f'Dyna-Q+: {n} planning steps')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Comparing Algorithms')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Steps per episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed2e9d",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Simple Monte-Carlo Search</h2>\n",
    "From the current state $ s $ we generate $ k $ episodes with the learned model.\n",
    "Afterwards we update the q-values from $ s, a $ with the mean reward from the generated episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f979cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Monte Carlo Search\n",
    "k = 10 # simulated episodes\n",
    "max_episode_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_monte_carlo_search(exercise, number_of_episodes, epsilon, gamma, alpha, simulated_episodes):\n",
    "    env = gym.make(exercise)\n",
    "    steps_per_episode = []\n",
    "    \n",
    "    # Initialization\n",
    "    q_map = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    transition_probabilities = np.zeros((env.nS, env.nS, env.nA))\n",
    "    transition_rewards = np.zeros((env.nS, env.nA))\n",
    "    state_action_state_transitions = np.zeros((env.nS, env.nS, env.nA))\n",
    "    state_action_transitions = np.zeros((env.nS, env.nA))\n",
    "    state_action_rewards = np.zeros((env.nS, env.nA))\n",
    "    visited_states = set()\n",
    "    \n",
    "    final_states = set()\n",
    "    \n",
    "    for e in range(1, number_of_episodes + 1):\n",
    "        episode_finished = False\n",
    "        s = env.reset()\n",
    "        step_counter = 0\n",
    "    \n",
    "        while not episode_finished:\n",
    "            step_counter += 1\n",
    "            a = choose_action_epsilon_greedy(s, epsilon, q_map, env.nA)\n",
    "            \n",
    "            s_next, reward, done, _ = env.step(a)\n",
    "            \n",
    "            if done and step_counter != 200:\n",
    "                final_states.add(s_next)\n",
    "            \n",
    "            episode_finished = done\n",
    "            \n",
    "            a_next = choose_action_greedy(s_next, q_map, env.nA)\n",
    "            \n",
    "            q_map[s, a] = q_map[s, a] + alpha * (reward + gamma * q_map[s_next, a_next] - q_map[s, a])\n",
    "            \n",
    "            # Update model\n",
    "            state_action_transitions[s, a] = state_action_transitions[s, a] + 1\n",
    "            state_action_state_transitions[s, s_next, a] = state_action_state_transitions[s, s_next, a] + 1\n",
    "            state_action_rewards[s, a] = state_action_rewards[s, a] + reward\n",
    "            \n",
    "            list_of_next_states = state_action_state_transitions[s, :, a]\n",
    "            for element in range(len(list_of_next_states)):\n",
    "                transition_probabilities[s, element, a] = state_action_state_transitions[s, element, a] / state_action_transitions[s, a]\n",
    "            \n",
    "            transition_rewards[s, a] = state_action_rewards[s, a] / state_action_transitions[s, a]\n",
    "            visited_states.add((s, a))\n",
    "            \n",
    "            # Planning\n",
    "            for a_index in range(env.nA):\n",
    "                if state_action_transitions[s, a_index] == 0:\n",
    "                    continue\n",
    "                for k in range(1, simulated_episodes+1):\n",
    "                    episode = simulate_full_episode(s_next, a_index, epsilon, final_states, transition_rewards, transition_probabilities, q_map)\n",
    "                    g = 0\n",
    "                    for t in reversed(range(len(episode))):\n",
    "                        step = episode[t]\n",
    "                        state = step[0]\n",
    "                        reward = step[2]\n",
    "                        g = gamma * g + reward\n",
    "                    q_map[s, a] = q_map[s, a] +  (1/simulated_episodes+1) * (g - q_map[s, a])\n",
    "                \n",
    "            s = s_next\n",
    "        steps_per_episode.append(step_counter)\n",
    "    return steps_per_episode\n",
    "\n",
    "def simulate_full_episode(state, action, epsilon, final_states, transition_rewards, transition_probabilities, q_map):\n",
    "    episode = []\n",
    "    while True:\n",
    "        reward = transition_rewards[state, action]\n",
    "        \n",
    "        # Mögliche Zielzustände\n",
    "        target_states = transition_probabilities[state, :, action]\n",
    "        if not np.any(target_states):\n",
    "            break\n",
    "        \n",
    "        s_next = np.random.choice(np.arange(target_states.size), p = target_states)\n",
    "        \n",
    "        episode.append([state, action, reward])\n",
    "        \n",
    "        # Nächsten State setzen\n",
    "        state = s_next\n",
    "        \n",
    "        # Nächste Action setzen\n",
    "        target_actions = q_map[state]\n",
    "        if not np.any(target_actions):\n",
    "            break\n",
    "        \n",
    "        if random.uniform(0, 1) <= epsilon:\n",
    "            action = random.randrange(0, 5)\n",
    "        else:\n",
    "            # Tie-breaking argmax\n",
    "            action = np.random.choice(np.flatnonzero(target_actions == target_actions.max()))\n",
    "        \n",
    "        if state in final_states:\n",
    "            episode.append([state, -1, 0])\n",
    "            break\n",
    "            \n",
    "        if len(episode) == max_episode_length:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plot_episode_statistics(simple_monte_carlo_search(exercise, episodes, epsilon, gamma, alpha, k), episodes, f'Simple Monte Carlo Search: {k} episode simulations')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Comparing Algorithms')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Steps per episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6b2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
