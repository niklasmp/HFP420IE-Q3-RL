{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Homework 3 - Model-free control</h1>\n",
    "In the last homework we dealt with model-free prediction. We evaluated a given policy. In this homework we deal with Model-free control. The goal is to optimize the policy to maximize return.\n",
    "<h1 style=\"text-align: center;\">Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import pandas as pd\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 style=\"text-align: center;\">Helper functions</h1>\n",
    "\n",
    "Helper functions such as epsilon greedy are required for Sarsa and Q-learning. We also need a helper function to evaluate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def choose_action_epsilon_greedy(state, epsilon, q_map, actions):\n",
    "    if random.uniform(0, 1) <= epsilon:\n",
    "        return random.randrange(0, actions)\n",
    "    else:\n",
    "        return np.argmax(q_map[state])\n",
    "\n",
    "def choose_action_greedy(state, q_map, actions):\n",
    "    return choose_action_epsilon_greedy(state, 0, q_map, actions)\n",
    "\n",
    "def print_greedy_policy(q_map, actions_utf_map, actions, grid_width = 4):\n",
    "    result = ''\n",
    "    for state in range(0, len(q_map)):\n",
    "        action = choose_action_greedy(state, q_map, actions)\n",
    "        result += '\\n' if state % grid_width == 0 else ' '\n",
    "        result += f'{actions_utf_map[action]} '\n",
    "    print(result)\n",
    "    \n",
    "def plot_episode_statistics(data_map, alpha, episodes, algorithm, exercise, ylabel):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for key, value in data_map.items():\n",
    "        df = pd.DataFrame({'x': np.arange(1, episodes+1), 'y': data_map[key]})\n",
    "        rolling_means = df.rolling(rolling_mean_size).mean()\n",
    "        plt.plot('x', 'y', f'{marker}{line}', data=rolling_means, label=f'alpha={alpha}, epsilon={key}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{exercise} - {algorithm[\"name\"]}')\n",
    "    plt.xlabel('# episodes')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_episode_statistics_sampled(data_map, alpha, episodes, algorithm, exercise, ylabel):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for key, value in data_map.items():\n",
    "        df = pd.DataFrame({'x': np.arange(1, episodes+1), 'y': data_map[key]})\n",
    "        df_sample = df.sample(sample_size).sort_index()\n",
    "        plt.plot('x', 'y', f'{marker}{line}', data=df_sample, label=f'alpha={alpha}, epsilon={key}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{exercise} - {algorithm[\"name\"]}')\n",
    "    plt.xlabel('# episodes')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Sarsa(0)</h1>\n",
    "\n",
    "SARSA is the most simple TD learning algorithm, named after <b>S</b>tate <b>a</b>ction <b>r</b>eward <b>s</b>tate <b>a</b>ction.\n",
    "\n",
    "Whenever we observe a reward, we update the previous q-value based on that reward and the discounted estimated value of the next action (that will be chosen under a policy based on q).\n",
    "\n",
    "In the current state s take an a, selected based on q-function. Therefore  we need a q_map with state and action, q(s, a). We update q(s, a) based on observed reward and estimated q-value for the next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sarsa_zero(exercise, number_of_episodes, epsilon, gamma, alpha):\n",
    "    env = gym.make(exercise)\n",
    "    action_count = env.nA\n",
    "    print(f'# states: {env.nS}')\n",
    "    print(f'# actions: {action_count}')\n",
    "    q_map = np.zeros((env.nS, env.nA))\n",
    "    successful_episodes = 0\n",
    "    step_counts = []\n",
    "    rewards = []\n",
    "    for episode in range(1, number_of_episodes + 1):\n",
    "        state = env.reset()\n",
    "        episode_finished = False\n",
    "        a = choose_action_epsilon_greedy(state, epsilon, q_map, action_count)\n",
    "        step_count = 0\n",
    "        cumulated_reward = 0\n",
    "        while not episode_finished:\n",
    "            new_state, reward, done, _ = env.step(a)\n",
    "            cumulated_reward += reward\n",
    "            if done and reward >= 1:\n",
    "                successful_episodes += 1\n",
    "            episode_finished = done\n",
    "            a_next = choose_action_epsilon_greedy(new_state, epsilon, q_map, action_count)\n",
    "            td_error = reward + gamma * q_map[new_state, a_next] - q_map[state, a]\n",
    "            q_map[state, a] = q_map[state, a] + alpha * td_error\n",
    "\n",
    "            state = new_state\n",
    "            a = a_next\n",
    "            step_count += 1\n",
    "        step_counts.append(step_count)\n",
    "        rewards.append(cumulated_reward)\n",
    "    print(f'Successful episodes: {successful_episodes}/{number_of_episodes} ({successful_episodes/number_of_episodes})')\n",
    "    print(f'Mean steps per episode: {statistics.mean(step_counts)}')\n",
    "    print(f'Median steps per episode: {statistics.median(step_counts)}')\n",
    "    return q_map, step_counts, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Sarsa(Î»)</h1>\n",
    "\n",
    "Sarsa lambda is very similar to sarsa. There is a difference in the use of state-action eligibility traces. For example accumulating traces, dutch traces and replacing traces.\n",
    "The updates to the q-function are based on the TD-error and the eligibility trace values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sarsa_lambda(exercise, number_of_episodes, epsilon, gamma, alpha, lambda_value):\n",
    "    env = gym.make(exercise)\n",
    "    action_count = env.nA\n",
    "    print(f'# states: {env.nS}')\n",
    "    print(f'# actions: {action_count}')\n",
    "    q_map = np.zeros((env.nS, env.nA))\n",
    "    successful_episodes = 0\n",
    "    step_counts = []\n",
    "    rewards = []\n",
    "    for episode in range(1, number_of_episodes + 1):\n",
    "        eligibility_traces = np.zeros((env.nS, env.nA))\n",
    "        state = env.reset()\n",
    "        episode_finished = False\n",
    "        a = choose_action_epsilon_greedy(state, epsilon, q_map, action_count)\n",
    "        step_count = 0\n",
    "        cumulated_reward = 0\n",
    "        while not episode_finished:\n",
    "            new_state, reward, done, _ = env.step(a)\n",
    "            cumulated_reward += reward\n",
    "            if done and reward >= 1:\n",
    "                successful_episodes += 1\n",
    "            episode_finished = done\n",
    "\n",
    "            a_next = choose_action_epsilon_greedy(new_state, epsilon, q_map, action_count)\n",
    "            delta = reward + gamma * q_map[new_state, a_next] - q_map[state, a]\n",
    "\n",
    "            eligibility_traces[state, a] += 1 # accumulating traces\n",
    "            # or eligibility_traces[state, a] = (1-alpha) * eligibility_traces[state, a] +1 # dutch traces\n",
    "            # or eligibility_traces[state, a] = 1 # replacing traces\n",
    "            for es in range(0, env.nS):\n",
    "                for ea in range (0, env.nA):\n",
    "                    q_map[state, a] = q_map[state, a] + alpha * delta * eligibility_traces[es, ea]\n",
    "                    eligibility_traces[es, ea] = gamma * lambda_value * eligibility_traces[es, ea]\n",
    "\n",
    "            state = new_state\n",
    "            a = a_next\n",
    "            step_count += 1        \n",
    "        step_counts.append(step_count)\n",
    "        rewards.append(cumulated_reward)\n",
    "    print(f'Successful episodes: {successful_episodes}/{number_of_episodes} ({successful_episodes/number_of_episodes})')\n",
    "    print(f'Mean steps per episode: {statistics.mean(step_counts)}')\n",
    "    print(f'Median steps per episode: {statistics.median(step_counts)}')\n",
    "    return q_map, step_counts, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Q-Learning</h1>\n",
    "\n",
    "Q-learning is similar to Sarsa. The key difference is that Sarsa is On-Policy and Q-learning is Off-Policy learning. The q-value of the action that would be selected by the target policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(exercise, number_of_episodes, epsilon, gamma, alpha):\n",
    "    env = gym.make(exercise)\n",
    "    action_count = env.nA\n",
    "    print(f'# states: {env.nS}')\n",
    "    print(f'# actions: {action_count}')\n",
    "    q_map = np.zeros((env.nS, env.nA))\n",
    "    successful_episodes = 0\n",
    "    step_counts = []\n",
    "    rewards = []\n",
    "    for episode in range(1, number_of_episodes + 1):\n",
    "        state = env.reset()\n",
    "        episode_finished = False;\n",
    "        step_count = 0\n",
    "        cumulated_reward = 0\n",
    "        while not episode_finished:\n",
    "            a = choose_action_epsilon_greedy(state, epsilon, q_map, action_count)\n",
    "\n",
    "            new_state, reward, done, _ = env.step(a)\n",
    "            cumulated_reward += reward\n",
    "            if done and reward >= 1:\n",
    "                successful_episodes += 1\n",
    "            episode_finished = done\n",
    "\n",
    "            a_next = choose_action_greedy(new_state, q_map, action_count)\n",
    "            td_error = reward + gamma * q_map[new_state, a_next] - q_map[state, a]\n",
    "            q_map[state, a] = q_map[state, a] + alpha * td_error\n",
    "\n",
    "            state = new_state\n",
    "            step_count += 1\n",
    "        step_counts.append(step_count)\n",
    "        rewards.append(cumulated_reward)\n",
    "    print(f'Successful episodes: {successful_episodes}/{number_of_episodes} ({successful_episodes/number_of_episodes})')\n",
    "    print(f'Mean steps per episode: {statistics.mean(step_counts)}')\n",
    "    print(f'Median steps per episode: {statistics.median(step_counts)}')\n",
    "    return q_map, step_counts, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Example problems</h1>\n",
    "\n",
    "The evolution of The Frozen Lake example and The Taxi example is shown below. We'll use the following parameters for both examples to better compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "episodes = 10000\n",
    "gamma = 1\n",
    "alphas = [0.2, 0.1, 0.05]\n",
    "epsilons= [0.1, 0.2, 0.3, 0.5]\n",
    "algorithms = [\n",
    "    {\n",
    "        \"name\": \"Sarsa(0)\",\n",
    "        \"q_map\": lambda alpha, epsilon: sarsa_zero(exercise, episodes, epsilon, gamma, alpha),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Sarsa(Î» = 0)\",\n",
    "        \"q_map\": lambda  alpha, epsilon: sarsa_lambda(exercise, episodes, epsilon, gamma, alpha, 0),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Sarsa(Î» = 0.5)\",\n",
    "        \"q_map\": lambda  alpha, epsilon: sarsa_lambda(exercise, episodes, epsilon, gamma, alpha, 0.5)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Sarsa(Î» = 1)\",\n",
    "        \"q_map\": lambda  alpha, epsilon: sarsa_lambda(exercise, episodes, epsilon, gamma, alpha, 1)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Q-Learning\",\n",
    "        \"q_map\": lambda alpha, epsilon: q_learning(exercise, episodes, epsilon, gamma, alpha)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Matplotlib\n",
    "sample_size = 200\n",
    "rolling_mean_size = 250\n",
    "marker = ','\n",
    "line = '-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">FrozenLake-v0</h2>\n",
    "\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "*Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.*\n",
    "\n",
    "*The surface is described using a grid like the following:*\n",
    "\n",
    "\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "Source: https://gym.openai.com/envs/FrozenLake-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exercise = 'FrozenLake-v0'\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "actions_utf_map = {\n",
    "    LEFT: 'â',\n",
    "    DOWN:  'â',\n",
    "    RIGHT:  'â',\n",
    "    UP: 'â',\n",
    "}\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    for alpha in alphas:\n",
    "        step_count_map = {}\n",
    "        rewards_map = {}\n",
    "        for epsilon in epsilons:\n",
    "            print(algorithm[\"name\"] + \" - Î±: \"+ str(alpha) + \" - É: \"+ str(epsilon))\n",
    "            q_map, step_counts, rewards = algorithm[\"q_map\"](alpha = alpha, epsilon = epsilon)\n",
    "            step_count_map[epsilon] = step_counts\n",
    "            rewards_map[epsilon] = rewards\n",
    "            print_greedy_policy(q_map, actions_utf_map, 4)\n",
    "            print(\"\\n\\n\")\n",
    "    \n",
    "        plot_episode_statistics(step_count_map, alpha, episodes, algorithm, exercise, 'steps per episode')\n",
    "        plot_episode_statistics(rewards_map, alpha, episodes, algorithm, exercise, 'reward per episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Taxi-v3</h1>\n",
    "\n",
    "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "Source: https://gym.openai.com/envs/Taxi-v3/\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# see https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py (l 134 ff.)\n",
    "x_factor = 100\n",
    "y_factor = 20\n",
    "\n",
    "# Calculates the positional part of the state\n",
    "def compute_positional_state_part(x, y):\n",
    "    return x * x_factor + y * y_factor\n",
    "\n",
    "\n",
    "def get_states_for_ride(passenger_location, passenger_destination):\n",
    "    states = []\n",
    "    for taxi_x in range(0, 5):\n",
    "        for taxi_y in range(0, 5):\n",
    "            state = compute_positional_state_part(taxi_x, taxi_y) + passenger_location * 4 + passenger_destination\n",
    "            states.append(state)\n",
    "    return states\n",
    "\n",
    "\n",
    "# prints the action the the highest value for each state that corresponds to the given passenger location and destination\n",
    "def print_taxi_policy(q_map, actions_utf_map, passenger_location, passenger_destination):\n",
    "    relevant_states = get_states_for_ride(passenger_location, passenger_destination)\n",
    "    relevant_q_values = operator.itemgetter(*relevant_states)(q_map)\n",
    "    print_greedy_policy(relevant_q_values, actions_utf_map, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exercise = 'Taxi-v3'\n",
    "\n",
    "# Actions\n",
    "SOUTH = 0\n",
    "NORTH = 1\n",
    "EAST = 2\n",
    "WEST = 3\n",
    "PICK_UP = 4\n",
    "DROP_OFF = 5\n",
    "\n",
    "actions_utf_map = {\n",
    "    SOUTH:  'â',\n",
    "    NORTH: 'â',\n",
    "    EAST: 'â',\n",
    "    WEST:  'â',\n",
    "    PICK_UP: 'P',\n",
    "    DROP_OFF: 'D',\n",
    "}\n",
    "\n",
    "## Passenger locations\n",
    "PASSENGER_AT_R = 0\n",
    "PASSENGER_AT_G = 1\n",
    "PASSENGER_AT_Y = 2\n",
    "PASSENGER_AT_B = 3\n",
    "PASSENGER_IN_TAXI = 4\n",
    "\n",
    "## Destinations\n",
    "DEST_R = 0\n",
    "DEST_G = 1\n",
    "DEST_Y = 2\n",
    "DEST_B = 3\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    for alpha in alphas:\n",
    "        step_count_map = {}\n",
    "        rewards_map = {}\n",
    "        for epsilon in epsilons:\n",
    "            print(algorithm[\"name\"] + \" - Î±: \"+ str(alpha) + \" - É: \"+ str(epsilon))\n",
    "            q_map, step_counts, rewards = algorithm[\"q_map\"](alpha = alpha, epsilon = epsilon)\n",
    "            step_count_map[epsilon] = step_counts\n",
    "            rewards_map[epsilon] = rewards\n",
    "\n",
    "            print('Passenger starts at B with target R')\n",
    "            print_taxi_policy(q_map, actions_utf_map, PASSENGER_AT_B, DEST_R)\n",
    "            print_taxi_policy(q_map, actions_utf_map, PASSENGER_IN_TAXI, DEST_R)\n",
    "\n",
    "\n",
    "            print('Passenger starts at B with target Y')\n",
    "            print_taxi_policy(q_map, actions_utf_map, PASSENGER_AT_B, DEST_Y)\n",
    "            print_taxi_policy(q_map, actions_utf_map, PASSENGER_IN_TAXI, DEST_Y)\n",
    "            print(\"\\n\\n\")\n",
    "    \n",
    "        plot_episode_statistics(step_count_map, alpha, episodes, algorithm, exercise, 'steps per episode')\n",
    "        plot_episode_statistics(rewards_map, alpha, episodes, algorithm, exercise, 'reward per episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOa8xzkEI/2Y9Ip8Uez16iN",
   "collapsed_sections": [],
   "name": "RL - Model free learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
